# version: '3.8'  # Version is obsolete, Docker Compose v2 handles this automatically

services:
  postgres:
    image: timescale/timescaledb:latest-pg15
    environment:
      POSTGRES_DB: loom
      POSTGRES_USER: loom
      POSTGRES_PASSWORD: loom
      TIMESCALEDB_TELEMETRY: 'off'
      # Fix authentication for external connections
      POSTGRES_HOST_AUTH_METHOD: md5
      # Ensure PostgreSQL listens on all addresses
      POSTGRES_INITDB_ARGS: "--auth-host=md5 --auth-local=trust"
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    ports:
      - "5432:5432"
    volumes:
      - ~/loom-data/timescaledb:/var/lib/postgresql/data
      - ./services/ingestion-api/migrations:/docker-entrypoint-initdb.d
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U loom -d loom"]
      interval: 10s
      timeout: 5s
      retries: 5
    command: |
      postgres
      -c listen_addresses='*'
      -c max_connections=200
      -c shared_buffers=256MB
      -c effective_cache_size=1GB
      -c maintenance_work_mem=64MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://kafka:9092
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:29092,PLAINTEXT_HOST://0.0.0.0:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_NUM_PARTITIONS: ${LOOM_KAFKA_DEFAULT_PARTITIONS:-3}
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    volumes:
      - ~/loom-data/kafka/data:/var/lib/kafka/data
    ports:
      - "9092:9092"
    depends_on:
      - zookeeper
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:9092", "--list"]
      interval: 30s
      timeout: 10s
      retries: 5

  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    volumes:
      - ~/loom-data/zookeeper/data:/var/lib/zookeeper/data
      - ~/loom-data/zookeeper/logs:/var/lib/zookeeper/log

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    ports:
      - "8081:8080"
    depends_on:
      - kafka

  ingestion-api:
    build:
      context: ./services/ingestion-api
      dockerfile: Dockerfile
      args:
        GIT_COMMIT: ${GIT_COMMIT:-unknown}
        GIT_BRANCH: ${GIT_BRANCH:-unknown}
        BUILD_DATE: ${BUILD_DATE:-unknown}
        BUILD_VERSION: ${BUILD_VERSION:-latest}
    environment:
      LOOM_DATABASE_URL: postgresql://loom:loom@postgres:5432/loom
      LOOM_KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      LOOM_HOST: 0.0.0.0
      LOOM_PORT: 8000
      LOOM_LOG_LEVEL: INFO
      LOOM_ENVIRONMENT: development
      LOOM_KAFKA_DEFAULT_PARTITIONS: ${LOOM_KAFKA_DEFAULT_PARTITIONS:-3}
      LOOM_KAFKA_DEFAULT_REPLICATION_FACTOR: ${LOOM_KAFKA_DEFAULT_REPLICATION_FACTOR:-1}
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    ports:
      - "8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/healthz"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # ONNX-based VAD service
  onnx-vad:
    build:
      context: ./services/onnx-vad
      dockerfile: Dockerfile
    environment:
      LOOM_DATABASE_URL: postgresql://loom:loom@postgres:5432/loom
      LOOM_KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      LOOM_KAFKA_INPUT_TOPIC: device.audio.raw
      LOOM_KAFKA_OUTPUT_TOPIC: media.audio.vad_filtered
      VAD_THRESHOLD: "0.5"
      VAD_MIN_SPEECH_DURATION_MS: "250"
      VAD_MIN_SILENCE_DURATION_MS: "100"
      LOOM_LOG_LEVEL: INFO
      LOOM_HOST: 0.0.0.0
      LOOM_PORT: 8001
      VAD_MODEL_DEVICE: cpu
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    volumes:
      - ~/loom-data/models:/tmp/cache
    ports:
      - "8001:8001"
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import httpx; r = httpx.get('http://localhost:8001/healthz'); r.raise_for_status()"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  scheduled-consumers:
    build:
      context: ./services/scheduled-consumers
      dockerfile: Dockerfile
    environment:
      LOOM_KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      LOOM_LOG_LEVEL: INFO
      LOOM_DEVICE_ID: local-dev-scheduled-consumers
      LOOM_EMAIL_CHECK_INTERVAL_MINUTES: 30
      LOOM_SOCIAL_MEDIA_CHECK_INTERVAL_MINUTES: 60
      LOOM_WEB_ACTIVITY_CHECK_INTERVAL_MINUTES: 15
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    depends_on:
      kafka:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import sys; sys.exit(0)"]
      interval: 30s
      timeout: 10s
      retries: 3

  onnx-asr:
    build:
      context: ./services/onnx-asr
      dockerfile: Dockerfile
    environment:
      LOOM_DATABASE_URL: postgresql://loom:loom@postgres:5432/loom
      LOOM_KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      LOOM_KAFKA_INPUT_TOPIC: device.audio.raw  # Process raw audio directly
      LOOM_KAFKA_OUTPUT_TOPIC: media.text.transcribed.words
      ASR_MODEL_DEVICE: cpu  # Use 'cuda' for GPU support
      ASR_MODEL_NAME: nvidia/parakeet-tdt-0.6b
      LOOM_LOG_LEVEL: INFO
      LOOM_HOST: 0.0.0.0
      LOOM_PORT: 8002
      # Hugging Face cache directory
      HF_HOME: /tmp/cache
      TRANSFORMERS_CACHE: /tmp/cache
      TORCH_HOME: /tmp/cache
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    volumes:
      - ~/loom-data/models:/tmp/cache
    ports:
      - "8002:8002"
    depends_on:
      kafka:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/healthz"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s  # Allow time for model download

  minicpm-vision:
    build:
      context: ./services/minicpm-vision
      dockerfile: Dockerfile
    environment:
      LOOM_KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      LOOM_KAFKA_INPUT_TOPICS: '["device.image.camera.raw", "device.video.screen.raw"]'
      LOOM_KAFKA_OUTPUT_TOPIC: media.image.analysis.minicpm_results
      LOOM_MODEL_DEVICE: cpu  # Use 'cuda' for GPU support
      LOOM_LOG_LEVEL: INFO
      LOOM_HOST: 0.0.0.0
      LOOM_PORT: 8000
      # Disable outlines cache to avoid permission issues
      OUTLINES_CACHE_DIR: /tmp/outlines_cache
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    ports:
      - "8003:8000"
    depends_on:
      kafka:
        condition: service_healthy
    volumes:
      - ~/loom-data/models:/home/loom/.cache  # Model cache directory
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/healthz', timeout=5)"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s  # Allow time for model download
    deploy:
      resources:
        limits:
          memory: 8G  # MiniCPM-V requires significant memory
        reservations:
          memory: 4G

  pipeline-monitor-api:
    build:
      context: ./services/pipeline-monitor-api
      dockerfile: Dockerfile.simple
    environment:
      NODE_ENV: production
      PORT: 8080
      LOOM_DATABASE_HOST: postgres
      LOOM_DATABASE_PORT: 5432
      LOOM_DATABASE_NAME: loom
      LOOM_DATABASE_USER: loom
      LOOM_DATABASE_PASSWORD: loom
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    ports:
      - "8082:8080"
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8080/health"]
      interval: 30s
      timeout: 5s
      retries: 3

  pipeline-monitor:
    build:
      context: ./services/pipeline-monitor
      dockerfile: Dockerfile
    environment:
      VITE_API_URL: http://localhost:8082
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    ports:
      - "3000:3000"
    depends_on:
      - pipeline-monitor-api

  # Data Fetcher Services
  hackernews-fetcher:
    build:
      context: ./services/hackernews-fetcher
      dockerfile: Dockerfile
    environment:
      # Kafka configuration (fetchers use KAFKA_BOOTSTRAP_SERVERS not LOOM_KAFKA_BOOTSTRAP_SERVERS)
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      KAFKA_TOPIC_PREFIX: ""  # No prefix, use full topic names
      LOOM_KAFKA_OUTPUT_TOPIC: external.hackernews.favorites.raw
      LOOM_DEVICE_ID: local-dev-hackernews-fetcher
      LOOM_LOG_LEVEL: INFO
      LOOM_FETCH_INTERVAL_MINUTES: 15
      # HackerNews credentials from .env file
      LOOM_HACKERNEWS_USERNAME: ${LOOM_HACKERNEWS_USERNAME:-}
      LOOM_HACKERNEWS_PASSWORD: ${LOOM_HACKERNEWS_PASSWORD:-}
      LOOM_HACKERNEWS_MAX_ITEMS: ${LOOM_HACKERNEWS_MAX_ITEMS:-100}
      LOOM_HACKERNEWS_FETCH_TYPE: ${LOOM_HACKERNEWS_FETCH_TYPE:-favorites}
      LOOM_HACKERNEWS_USE_BROWSER: ${LOOM_HACKERNEWS_USE_BROWSER:-false}
      LOOM_HACKERNEWS_RATE_LIMIT_SECONDS: ${LOOM_HACKERNEWS_RATE_LIMIT_SECONDS:-0.5}
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    volumes:
      - ~/loom-data/logs/hackernews:/app/logs
    depends_on:
      kafka:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import sys; sys.exit(0)"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  gmail-fetcher:
    build:
      context: ./services/email-fetcher
      dockerfile: Dockerfile
    environment:
      # Kafka configuration (fetchers use KAFKA_BOOTSTRAP_SERVERS not LOOM_KAFKA_BOOTSTRAP_SERVERS)
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      KAFKA_TOPIC_PREFIX: ""  # No prefix, use full topic names
      LOOM_KAFKA_OUTPUT_TOPIC: external.email.events.raw
      LOOM_DEVICE_ID: ${LOOM_DEVICE_ID_GMAIL:-local-dev-gmail-fetcher}
      LOOM_LOG_LEVEL: ${LOOM_LOG_LEVEL:-INFO}
      LOOM_FETCH_INTERVAL_MINUTES: ${LOOM_FETCH_INTERVAL_MINUTES_GMAIL:-5}
      # Gmail configuration from .env file
      LOOM_EMAIL_ADDRESS_1: ${LOOM_EMAIL_1:-}
      LOOM_EMAIL_PASSWORD_1: ${LOOM_EMAIL_PASSWORD_1:-}
      LOOM_EMAIL_IMAP_SERVER_1: ${LOOM_EMAIL_IMAP_SERVER_1:-imap.gmail.com}
      LOOM_EMAIL_IMAP_PORT_1: ${LOOM_EMAIL_PORT_1:-993}
      LOOM_EMAIL_NAME_1: ${LOOM_EMAIL_NAME_1:-Gmail}
      # Fetch ALL emails instead of just UNSEEN
      LOOM_EMAIL_SEARCH_CRITERIA: ALL
      LOOM_EMAIL_MAX_FETCH_PER_ACCOUNT: 1000
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    depends_on:
      kafka:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import sys; sys.exit(0)"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  fastmail-fetcher:
    build:
      context: ./services/email-fetcher
      dockerfile: Dockerfile
    environment:
      # Kafka configuration (fetchers use KAFKA_BOOTSTRAP_SERVERS not LOOM_KAFKA_BOOTSTRAP_SERVERS)
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      KAFKA_TOPIC_PREFIX: ""  # No prefix, use full topic names
      LOOM_KAFKA_OUTPUT_TOPIC: external.email.events.raw
      LOOM_DEVICE_ID: ${LOOM_DEVICE_ID_FASTMAIL:-local-dev-fastmail-fetcher}
      LOOM_LOG_LEVEL: ${LOOM_LOG_LEVEL:-INFO}
      LOOM_FETCH_INTERVAL_MINUTES: ${LOOM_FETCH_INTERVAL_MINUTES_FASTMAIL:-5}
      # Fastmail configuration from .env file (as account 1 for this fetcher)
      LOOM_EMAIL_ADDRESS_1: ${LOOM_EMAIL_2:-}
      LOOM_EMAIL_PASSWORD_1: ${LOOM_EMAIL_PASSWORD_2:-}
      LOOM_EMAIL_IMAP_SERVER_1: ${LOOM_EMAIL_IMAP_SERVER_2:-imap.fastmail.com}
      LOOM_EMAIL_IMAP_PORT_1: ${LOOM_EMAIL_PORT_2:-993}
      LOOM_EMAIL_NAME_1: ${LOOM_EMAIL_NAME_2:-Fastmail}
      # Fetch ALL emails instead of just UNSEEN
      LOOM_EMAIL_SEARCH_CRITERIA: ALL
      LOOM_EMAIL_MAX_FETCH_PER_ACCOUNT: 1000
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    depends_on:
      kafka:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import sys; sys.exit(0)"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  calendar-fetcher:
    build:
      context: ./services/calendar-fetcher
      dockerfile: Dockerfile
    environment:
      # Kafka configuration (fetchers use KAFKA_BOOTSTRAP_SERVERS not LOOM_KAFKA_BOOTSTRAP_SERVERS)
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      KAFKA_TOPIC_PREFIX: ""  # No prefix, use full topic names
      LOOM_KAFKA_OUTPUT_TOPIC: ${LOOM_KAFKA_OUTPUT_TOPIC_CALENDAR:-external.calendar.events.raw}
      LOOM_DEVICE_ID: ${LOOM_DEVICE_ID_CALENDAR:-local-dev-calendar-fetcher}
      LOOM_LOG_LEVEL: ${LOOM_LOG_LEVEL:-INFO}
      LOOM_FETCH_INTERVAL_MINUTES: ${LOOM_FETCH_INTERVAL_MINUTES_CALENDAR:-10}
      # CalDAV configuration from .env file
      LOOM_CALDAV_URL_1: ${LOOM_CALDAV_URL_1:-}
      LOOM_CALDAV_USERNAME_1: ${LOOM_CALDAV_USERNAME_1:-}
      LOOM_CALDAV_PASSWORD_1: ${LOOM_CALDAV_PASSWORD_1:-}
      LOOM_CALDAV_NAME_1: ${LOOM_CALDAV_NAME_1:-}
      LOOM_CALDAV_DISABLED_1: ${LOOM_CALDAV_DISABLED_1:-false}
      # Calendar fetching configuration
      LOOM_CALENDAR_DAYS_PAST: "365"  # Fetch events from the past year (increased from default 30)
      LOOM_CALENDAR_DAYS_FUTURE: "365"  # Fetch events up to 1 year in the future
      LOOM_CALENDAR_MAX_ACCOUNTS: "10"  # Support up to 10 calendar accounts
      LOOM_CALENDAR_ENABLE_GPS_LOOKUP: "true"  # Enable GPS coordinate lookup for locations
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    depends_on:
      kafka:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import sys; sys.exit(0)"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  x-likes-fetcher:
    build:
      context: ./services/x-likes-fetcher
      dockerfile: Dockerfile
    environment:
      # Kafka configuration (fetchers use KAFKA_BOOTSTRAP_SERVERS not LOOM_KAFKA_BOOTSTRAP_SERVERS)
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      KAFKA_TOPIC_PREFIX: ""  # No prefix, use full topic names
      LOOM_KAFKA_OUTPUT_TOPIC: ${LOOM_KAFKA_OUTPUT_TOPIC_X:-external.twitter.liked.raw}
      LOOM_KAFKA_URL_TOPIC: ${LOOM_KAFKA_URL_TOPIC_X:-task.url.ingest}
      LOOM_SEND_TO_URL_PROCESSOR: ${LOOM_SEND_TO_URL_PROCESSOR_X:-true}
      LOOM_DEVICE_ID: local-dev-x-likes-fetcher
      LOOM_LOG_LEVEL: INFO
      LOOM_FETCH_INTERVAL_HOURS: ${LOOM_FETCH_INTERVAL_HOURS_X:-6}
      LOOM_RUN_ON_STARTUP: ${LOOM_RUN_ON_STARTUP_X:-true}
      LOOM_MAX_TWEETS_TO_FETCH: 1000  # Fetch up to 1000 tweets
      LOOM_MAX_FETCH_TIME_MINUTES: 120  # Stop after 2 hours
      # Database configuration for duplicate checking
      LOOM_DATABASE_URL: postgresql://loom:loom@postgres:5432/loom
      # X/Twitter authentication (from .env file)
      LOOM_X_USERNAME: ${X_USERNAME:-}
      LOOM_X_PASSWORD: ${X_PASSWORD:-}
      LOOM_X_PHONE_NUMBER: ${X_PHONE_NUMBER:-}
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    volumes:
      - ~/loom-data/x-likes-sessions:/app/sessions
      - ~/loom-data/logs/x-likes:/app/logs
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import sys; sys.exit(0)"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # GPS Geocoding Consumer
  gps-geocoding-consumer:
    build:
      context: ./services/gps-geocoding-consumer
      dockerfile: Dockerfile
    environment:
      LOOM_KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      LOOM_KAFKA_CONSUMER_GROUP_ID: gps-geocoding-consumer
      LOOM_KAFKA_INPUT_TOPIC: device.sensor.gps.raw
      LOOM_KAFKA_OUTPUT_TOPIC: location.address.geocoded
      LOOM_DATABASE_URL: postgresql://loom:loom@postgres:5432/loom
      LOOM_OPENCAGE_API_KEY: ${LOOM_OPENCAGE_API_KEY:-}  # Set via environment variable
      LOOM_MIN_DISTANCE_METERS: 100.0  # Only geocode if 100m+ from cached points
      LOOM_CACHE_RADIUS_METERS: 50.0   # Search radius for cache hits
      LOOM_LOG_LEVEL: DEBUG
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    ports:
      - "8012:8000"  # Health check and metrics endpoint
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  # Significant Motion Detection Consumer
  significant-motion-detector:
    build:
      context: ./services/significant-motion-detector
      dockerfile: Dockerfile
    environment:
      LOOM_KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      LOOM_KAFKA_CONSUMER_GROUP_ID: significant-motion-detector
      LOOM_KAFKA_INPUT_TOPIC: device.sensor.accelerometer.raw
      LOOM_KAFKA_OUTPUT_TOPIC: motion.events.significant
      LOOM_DATABASE_URL: postgresql://loom:loom@postgres:5432/loom
      LOOM_LOG_LEVEL: INFO
      # Motion detection settings
      LOOM_MOTION_WINDOW_SECONDS: "2.0"
      LOOM_MOTION_THRESHOLD_MS2: "2.0"
      LOOM_MOTION_MIN_DURATION_SECONDS: "0.5"
      LOOM_MOTION_COOLDOWN_SECONDS: "5.0"
      # Activity thresholds
      LOOM_ACTIVITY_WALKING_THRESHOLD: "2.0"
      LOOM_ACTIVITY_RUNNING_THRESHOLD: "5.0"
      LOOM_ACTIVITY_VEHICLE_THRESHOLD: "1.0"
      LOOM_ACTIVITY_STATIONARY_THRESHOLD: "0.5"
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    ports:
      - "8013:8013"  # Health check and metrics endpoint
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8013/healthz', timeout=5)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  # Step Counter Consumer
  step-counter:
    build:
      context: ./services/step-counter
      dockerfile: Dockerfile
    environment:
      LOOM_KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      LOOM_KAFKA_CONSUMER_GROUP_ID: step-counter
      LOOM_KAFKA_INPUT_TOPIC: device.sensor.accelerometer.raw
      LOOM_KAFKA_OUTPUT_TOPIC: device.health.steps.raw
      LOOM_LOG_LEVEL: INFO
      # Step detection settings
      LOOM_STEP_WINDOW_SECONDS: "5.0"
      LOOM_STEP_THRESHOLD_MS2: "1.5"
      LOOM_STEP_FREQUENCY_MIN_HZ: "0.5"
      LOOM_STEP_FREQUENCY_MAX_HZ: "3.0"
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    ports:
      - "8014:8014"  # Health check endpoint
    depends_on:
      kafka:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8014/healthz', timeout=5)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  # Activity Classifier Consumer
  activity-classifier:
    build:
      context: ./services/activity-classifier
      dockerfile: Dockerfile
    environment:
      LOOM_KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      LOOM_KAFKA_CONSUMER_GROUP_ID: activity-classifier
      LOOM_KAFKA_INPUT_TOPICS: "motion.events.significant,device.sensor.gps.raw,device.health.steps.raw"
      LOOM_KAFKA_OUTPUT_TOPIC: motion.classification.activity
      LOOM_LOG_LEVEL: INFO
      # Classification settings
      LOOM_CLASSIFICATION_WINDOW_SECONDS: "60.0"
      LOOM_LOCATION_RADIUS_METERS: "100.0"
      # Activity thresholds
      LOOM_ACTIVITY_MIN_STEPS_PER_MINUTE: "30"
      LOOM_ACTIVITY_RUNNING_STEPS_PER_MINUTE: "120"
      LOOM_ACTIVITY_VEHICLE_SPEED_MS: "5.0"
      LOOM_ACTIVITY_STATIONARY_SPEED_MS: "0.5"
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    ports:
      - "8015:8015"  # Health check endpoint
    depends_on:
      kafka:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8015/healthz', timeout=5)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped

  # Processing Services
  hackernews-url-processor:
    build:
      context: ./services/hackernews-url-processor
      dockerfile: Dockerfile
    environment:
      # Kafka configuration
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      KAFKA_TOPIC_PREFIX: ""  # No prefix, use full topic names
      LOOM_KAFKA_INPUT_TOPIC: external.hackernews.favorites.raw
      LOOM_KAFKA_OUTPUT_TOPIC: task.url.processed.hackernews_archived
      LOOM_LOG_LEVEL: INFO
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    volumes:
      - ~/loom-data/logs/hackernews-processor:/app/logs
    depends_on:
      kafka:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import sys; sys.exit(0)"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  x-url-processor:
    build:
      context: ./services/x-url-processor
      dockerfile: Dockerfile
    environment:
      # Kafka configuration
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      KAFKA_TOPIC_PREFIX: ""  # No prefix, use full topic names
      LOOM_KAFKA_INPUT_TOPIC: external.twitter.liked.raw  # Only process from Twitter liked topic
      LOOM_KAFKA_OUTPUT_TOPIC: task.url.processed.twitter_archived
      LOOM_LOG_LEVEL: INFO
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    volumes:
      - ~/loom-data/twitter_images:/app/screenshots
      - ~/loom-data/logs/x-url-processor:/app/logs
    depends_on:
      kafka:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import sys; sys.exit(0)"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Database Consumer (Legacy) - DISABLED in favor of generic-kafka-to-db-consumer
  # kafka-to-db-consumer:
  #   build:
  #     context: ./services/kafka-to-db-consumer
  #     dockerfile: Dockerfile
  #   environment:
  #     LOOM_KAFKA_BOOTSTRAP_SERVERS: kafka:29092
  #     LOOM_DATABASE_URL: postgresql://loom:loom@postgres:5432/loom
  #     LOOM_LOG_LEVEL: INFO
  #     CONSUMER_TYPE: legacy  # Use legacy consumer
  #     # Set timezone
  #     TZ: UTC
  #     DEBIAN_FRONTEND: noninteractive
  #   volumes:
  #     - ~/loom-data/logs/kafka-to-db:/app/logs
  #   depends_on:
  #     postgres:
  #       condition: service_healthy
  #     kafka:
  #       condition: service_healthy
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8001/healthz"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3
  #   restart: unless-stopped

  # Database Consumer (mapping-based, reads config from database)
  # Instance 1
  kafka-to-db-consumer-1:
    build:
      context: ./services
      dockerfile: kafka-to-db-consumer/Dockerfile
    image: loomv2-kafka-to-db-consumer:latest
    environment:
      LOOM_KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      LOOM_DATABASE_URL: postgresql://loom:loom@postgres:5432/loom
      LOOM_LOG_LEVEL: INFO
      CONSUMER_TYPE: generic  # Use new generic consumer
      KAFKA_GROUP_ID: kafka-to-db-consumer
      USE_DATABASE_CONFIG: "true"  # Source topics from database instead of YAML
      CONSUMER_INSTANCE_ID: "1"
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    ports:
      - "8009:8001"
    volumes:
      - ~/loom-data/logs/kafka-to-db-1:/app/logs
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Instance 2
  kafka-to-db-consumer-2:
    build:
      context: ./services
      dockerfile: kafka-to-db-consumer/Dockerfile
    image: loomv2-kafka-to-db-consumer:latest
    environment:
      LOOM_KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      LOOM_DATABASE_URL: postgresql://loom:loom@postgres:5432/loom
      LOOM_LOG_LEVEL: INFO
      CONSUMER_TYPE: generic  # Use new generic consumer
      KAFKA_GROUP_ID: kafka-to-db-consumer
      USE_DATABASE_CONFIG: "true"  # Source topics from database instead of YAML
      CONSUMER_INSTANCE_ID: "2"
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    ports:
      - "8010:8001"
    volumes:
      - ~/loom-data/logs/kafka-to-db-2:/app/logs
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Instance 3
  kafka-to-db-consumer-3:
    build:
      context: ./services
      dockerfile: kafka-to-db-consumer/Dockerfile
    image: loomv2-kafka-to-db-consumer:latest
    environment:
      LOOM_KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      LOOM_DATABASE_URL: postgresql://loom:loom@postgres:5432/loom
      LOOM_LOG_LEVEL: INFO
      CONSUMER_TYPE: generic  # Use new generic consumer
      KAFKA_GROUP_ID: kafka-to-db-consumer
      USE_DATABASE_CONFIG: "true"  # Source topics from database instead of YAML
      CONSUMER_INSTANCE_ID: "3"
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    ports:
      - "8011:8001"
    volumes:
      - ~/loom-data/logs/kafka-to-db-3:/app/logs
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Instance 4
  kafka-to-db-consumer-4:
    build:
      context: ./services
      dockerfile: kafka-to-db-consumer/Dockerfile
    image: loomv2-kafka-to-db-consumer:latest
    environment:
      LOOM_KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      LOOM_DATABASE_URL: postgresql://loom:loom@postgres:5432/loom
      LOOM_LOG_LEVEL: INFO
      CONSUMER_TYPE: generic  # Use new generic consumer
      KAFKA_GROUP_ID: kafka-to-db-consumer
      USE_DATABASE_CONFIG: "true"  # Source topics from database instead of YAML
      CONSUMER_INSTANCE_ID: "4"
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    ports:
      - "8019:8001"
    volumes:
      - ~/loom-data/logs/kafka-to-db-4:/app/logs
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Text Embedder Service
  text-embedder:
    build:
      context: ./services/text-embedder
      dockerfile: Dockerfile
    environment:
      LOOM_KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      LOOM_DATABASE_URL: postgresql://loom:loom@postgres:5432/loom
      LOOM_KAFKA_EMAIL_TOPIC: external.email.events.raw
      LOOM_KAFKA_TWITTER_TOPIC: external.twitter.liked.raw
      LOOM_KAFKA_EMBEDDED_EMAIL_TOPIC: analysis.text.embedded.emails
      LOOM_KAFKA_EMBEDDED_TWITTER_TOPIC: analysis.text.embedded.twitter
      LOOM_EMBEDDING_MODEL: sentence-transformers/all-MiniLM-L6-v2
      LOOM_DEVICE: cpu
      LOOM_LOG_LEVEL: INFO
      LOOM_HOST: 0.0.0.0
      LOOM_PORT: 8006
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    ports:
      - "8006:8006"
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_healthy
    volumes:
      - ~/loom-data/models:/models  # Model cache directory
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8006/healthz"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s  # Allow time for model download
    restart: unless-stopped

  # Additional AI Services
  # bud-e-emotion service removed - can be re-added in future for emotion analysis
  # Reference: https://huggingface.co/laion/BUD-E-Whisper

  face-emotion:
    build:
      context: ./services/face-emotion
      dockerfile: Dockerfile
    environment:
      LOOM_KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      LOOM_KAFKA_INPUT_TOPIC: media.image.analysis.minicpm_results
      LOOM_KAFKA_OUTPUT_TOPIC: analysis.image.emotion_results
      LOOM_MODEL_NAME: trpakov/vit-face-expression  # Public facial emotion model
      LOOM_MODEL_DEVICE: cpu  # Use 'cuda' for GPU support
      LOOM_LOG_LEVEL: INFO
      LOOM_HOST: 0.0.0.0
      LOOM_PORT: 8005
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    ports:
      - "8005:8005"
    depends_on:
      kafka:
        condition: service_healthy
    volumes:
      - ~/loom-data/models:/home/loom/.cache  # Model cache directory
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8005/healthz', timeout=5)"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s  # Allow time for model download
    restart: unless-stopped

  moondream-ocr:
    build:
      context: ./services/moondream-ocr
      dockerfile: Dockerfile
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      PORT: 8007
      MOONDREAM_MODEL_CACHE: /home/aoi/.loom/moondream
      HF_HOME: /home/aoi/.loom/moondream
      TORCH_HOME: /home/aoi/.loom/moondream
      TRANSFORMERS_CACHE: /home/aoi/.loom/moondream
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    ports:
      - "8007:8007"
    depends_on:
      kafka:
        condition: service_healthy
    volumes:
      - ~/.loom:/home/aoi/.loom  # Model cache directory
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8007/healthz', timeout=5)"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s  # Allow time for model download
    deploy:
      resources:
        limits:
          memory: 8G  # Moondream requires significant memory
        reservations:
          memory: 4G
    restart: unless-stopped

  twitter-ocr-processor:
    build:
      context: ./services/twitter-ocr-processor
      dockerfile: Dockerfile
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      DATABASE_URL: postgresql://loom:loom@postgres:5432/loom
      MOONDREAM_OCR_URL: http://moondream-ocr:8007
      LOG_LEVEL: INFO
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_healthy
      moondream-ocr:
        condition: service_started
    restart: unless-stopped

  kafka-test-producer:
    build:
      context: ./services/kafka-test-producer
      dockerfile: Dockerfile
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      LOG_LEVEL: INFO
      # Set timezone
      TZ: UTC
      DEBIAN_FRONTEND: noninteractive
    ports:
      - "8008:8008"
    depends_on:
      kafka:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8008/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # onefilellm:  # Temporarily disabled due to GitHub access issues
  #   build:
  #     context: ./services/onefilellm
  #     dockerfile: Dockerfile
  #   environment:
  #     KAFKA_BOOTSTRAP_SERVERS: kafka:29092
  #     DATABASE_URL: postgresql://loom:loom@postgres:5432/loom
  #     TOPIC_GITHUB_INGEST: task.github.ingest
  #     TOPIC_DOCUMENT_INGEST: task.document.ingest
  #     TOPIC_GITHUB_PARSED: processed.github.parsed
  #     TOPIC_DOCUMENT_PARSED: processed.document.parsed
  #     LOG_LEVEL: INFO
  #     HOST: 0.0.0.0
  #     PORT: 8080
  #     MAX_FILE_SIZE_MB: 100
  #     MAX_REPO_FILES: 1000
  #     GITHUB_CLONE_TIMEOUT: 300
  #     PROCESSING_TIMEOUT: 600
  #   ports:
  #     - "8080:8080"
  #   depends_on:
  #     postgres:
  #       condition: service_healthy
  #     kafka:
  #       condition: service_healthy
  #   volumes:
  #     - ./temp_repos:/tmp/repos  # Temporary directory for cloning repos
  #   healthcheck:
  #     test: ["CMD", "python", "-c", "import httpx; r = httpx.get('http://localhost:8080/health'); r.raise_for_status()"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #     start_period: 60s  # Allow time for OneFileLLM setup
  #   deploy:
  #     resources:
  #       limits:
  #         memory: 4G  # OneFileLLM can be memory intensive
  #       reservations:
  #         memory: 2G

# volumes:
#   postgres_data: # Now using host directory ~/loom-data/timescaledb
