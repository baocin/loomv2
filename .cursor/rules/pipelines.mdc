---
description: 
globs: 
alwaysApply: false
---
**Audio Processing Pipeline:**
- **Input:** `device.audio.raw`
- **VAD Filtering (Silero VAD Consumer):**
  - **Purpose:** Identify speech segments and filter out non-speech.
  - **Output:** `media.audio.vad_filtered` (only speech chunks).
- **Transcription (Parakeet Consumer):**
  - **Model:** NVIDIA Parakeet TDT (https://huggingface.co/spaces/nvidia/parakeet-tdt-0.6b-v2)
  - **Purpose:** Transcribe speech audio to text, word-by-word, including timestamps for each word.
  - **Output:** `media.text.transcribed.words`

**One-Off Image Analysis Pipeline:**
- **Input:** `device.image.camera.raw` (screenshots, periodic webcam photos).
- **Consumer (Moondream/Object Detection):**
  - **Model:** Moondream (https://huggingface.co/moondream/moondream-2b-2025-04-14-4bit) for captions, gaze, common objects.
  - **Purpose:** Extract high-level visual context.
  - **Output:** `media.image.analysis.moondream_results`

**Streaming Video Analysis Pipeline:**
- **Input:** `device.video.screen.raw` (keyframes from screen recordings).
- **Consumer (YOLO/Optical Flow):**
  - **Models:** Ultralytics YOLO (https://github.com/ultralytics/ultralytics) for object detection, Optical Flow for motion analysis.
  - **Purpose:** Analyze motion and objects within continuous video streams.
  - **Output:** `media.video.analysis.yolo_results`

**3D Reconstruction Pipeline:**
- **Input:** `device.image.camera.raw` (specifically paired front and rear camera photos).
- **Consumer (DUST-R):**
  - **Model:** dust3r (https://github.com/naver/dust3r - or similar models for 3D reconstruction like DUST-R or Instant-NGP variants).
  - **Redis Usage:** This consumer will likely use Redis to temporarily store and correlate incoming front/rear frames by timestamp and device ID until a complete pair is available for processing.
  - **Purpose:** Reconstruct 3D environments from simultaneous camera views.
  - **Output:** `analysis.3d_reconstruction.dustr_results`

**Speculative Context Inference Pipeline:**
- **Input:** Samples from `device.audio.raw`, `device.image.camera.raw`.
- **Consumer (Qwen2.5-Omni-7B Sampler):**
  - **Model:** Qwen2.5-Omni-7B (https://huggingface.co/Qwen/Qwen2.5-Omni-7B)
  - **Purpose:** Run on *raw, unprocessed* samples to infer high-level context (e.g., guess location from background audio, identify if a photo is of food/people before specific object detection). This adds a valuable "sense-making" layer early in the process.
  - **Output:** `analysis.inferred_context.qwen_results`

**Generic URL Task Processing Pipeline:**
- **Input:** `task.url.ingest`
- **Consumer (Twitter Archiver):**
  - **Purpose:** Scrape and archive details of liked Twitter/X posts.
  - **Output:** `task.url.processed.twitter_archived`
- **Consumer (PDF Processor - onefilellm):**
  - **Model:** onefilellm (https://github.com/jimmc414/onefilellm)
  - **Purpose:** Extract text, summarize, or analyze content from PDF URLs (including base64 encoded).
  - **Output:** `task.url.processed.pdf_extracted`

**Unified Persistence Consumer(s):**
- **Input:** All final processed topics (e.g., `media.text.transcribed.words`, `media.image.analysis.moondream_results`, `analysis.inferred_context.qwen_results`, etc.) and some raw topics that don't need complex processing (e.g., `device.sensor.gps.raw`, `device.health.heartrate.raw`).
- **Purpose:** Ingest data into **TimescaleDB**. Design your TimescaleDB schema to be flexible and highly granular, leveraging hypertable capabilities for efficient time-series storage and querying. Each data type will likely have its own table (e.g., `transcripts`, `image_analyses`, `gps_data`, `device_states`).
